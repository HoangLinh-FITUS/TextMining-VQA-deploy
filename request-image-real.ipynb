{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:15:50.841641Z",
     "iopub.status.busy": "2025-01-16T17:15:50.841274Z",
     "iopub.status.idle": "2025-01-16T17:15:51.007831Z",
     "shell.execute_reply": "2025-01-16T17:15:51.006773Z",
     "shell.execute_reply.started": "2025-01-16T17:15:50.841602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import mimetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:15:51.009886Z",
     "iopub.status.busy": "2025-01-16T17:15:51.009584Z",
     "iopub.status.idle": "2025-01-16T17:16:18.367990Z",
     "shell.execute_reply": "2025-01-16T17:16:18.366909Z",
     "shell.execute_reply.started": "2025-01-16T17:15:51.009862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "import shutil\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:18.370024Z",
     "iopub.status.busy": "2025-01-16T17:16:18.369365Z",
     "iopub.status.idle": "2025-01-16T17:16:20.735769Z",
     "shell.execute_reply": "2025-01-16T17:16:20.734583Z",
     "shell.execute_reply.started": "2025-01-16T17:16:18.369975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9173a25293bd43478bdfc3c7c9353a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2954df97274a8aa56fa26ef5ae0351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3220f111990348a8a4dde7ba293dac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.737122Z",
     "iopub.status.busy": "2025-01-16T17:16:20.736717Z",
     "iopub.status.idle": "2025-01-16T17:16:20.742477Z",
     "shell.execute_reply": "2025-01-16T17:16:20.741320Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.737080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"image_fetch.log\"),  \n",
    "        logging.StreamHandler()               \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.743882Z",
     "iopub.status.busy": "2025-01-16T17:16:20.743546Z",
     "iopub.status.idle": "2025-01-16T17:16:20.760146Z",
     "shell.execute_reply": "2025-01-16T17:16:20.759015Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.743856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GI_API_KEY = os.getenv('API_KEY', 'AIzaSyA43pbijmUNCtMNgSopT7VOimtgERBRXKU')\n",
    "GI_SEARCH_ENGINE_ID = os.getenv('SEARCH_ENGINE_ID', 'b6f2650fd0921483a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.763461Z",
     "iopub.status.busy": "2025-01-16T17:16:20.763137Z",
     "iopub.status.idle": "2025-01-16T17:16:20.778100Z",
     "shell.execute_reply": "2025-01-16T17:16:20.777032Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.763434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PEXELS_API_KEY = os.getenv('API_KEY', 'yxshO7kOwkkbsGf2TmXkfq2MqWaMYjdaVOja0elnSPBXPgBL645wyYhs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.780100Z",
     "iopub.status.busy": "2025-01-16T17:16:20.779705Z",
     "iopub.status.idle": "2025-01-16T17:16:20.806095Z",
     "shell.execute_reply": "2025-01-16T17:16:20.805083Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.780062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_database(db_path=\"image_embeddings.db\"):\n",
    "    \"\"\"\n",
    "    Initialize a database at the given path, creating a table if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): Path to the database file. Defaults to \"image_embeddings.db\".\n",
    "\n",
    "    Returns:\n",
    "        sqlite3.Connection: The established connection to the database.\n",
    "    \"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS embeddings (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            url TEXT UNIQUE,\n",
    "            embedding BLOB\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def copy_existing_data(input_folder, output_folder, db_file):\n",
    "    \"\"\"Copy existing images and database to a writable output folder.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(input_folder):\n",
    "        shutil.copytree(input_folder, output_folder, dirs_exist_ok=True)\n",
    "    db_input = os.path.join(input_folder, db_file)\n",
    "    db_output = os.path.join(output_folder, db_file)\n",
    "    if os.path.exists(db_input):\n",
    "        shutil.copy(db_input, db_output)\n",
    "    return db_output\n",
    "\n",
    "def save_embedding_to_db(conn, url, embedding):\n",
    "    \"\"\"Save an image embedding to the database.\n",
    "\n",
    "    Args:\n",
    "        conn (sqlite3.Connection): Connection to the database.\n",
    "        url (str): URL of the image.\n",
    "        embedding (numpy.ndarray): The image embedding to save.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    embedding_blob = pickle.dumps(embedding)  \n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO embeddings (url, embedding) VALUES (?, ?)\", (url, embedding_blob))\n",
    "        conn.commit()\n",
    "    except sqlite3.IntegrityError:\n",
    "        logging.info(f\"URL already exists in the database: {url}\")\n",
    "\n",
    "def is_similar_to_existing(conn, new_embedding, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Check if the given embedding is similar to any existing embedding in the database.\n",
    "    \n",
    "    Args:\n",
    "        conn (sqlite3.Connection): Connection to the database.\n",
    "        new_embedding (numpy.ndarray): The embedding to check.\n",
    "        threshold (float, optional): The maximum cosine similarity between the new embedding and an existing one.\n",
    "            Defaults to 0.1.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the new embedding is similar to an existing one, False otherwise.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT embedding FROM embeddings\")\n",
    "    for row in cursor.fetchall():\n",
    "        existing_embedding = pickle.loads(row[0])  \n",
    "        similarity = 1 - cosine(new_embedding, existing_embedding)\n",
    "        if similarity > (1 - threshold):  \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"\n",
    "    Generate an embedding for an image using a Vision Transformer (ViT).\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file for which the embedding is to be generated.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 1D array representing the image embedding generated by the ViT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the output embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "\n",
    "    # Convert to numpy array\n",
    "    return embedding.squeeze().numpy()\n",
    "\n",
    "def load_page_state(state_file=\"page_state.json\"):\n",
    "    \"\"\"\n",
    "    Load the page state from a file to track which pages have been crawled.\n",
    "\n",
    "    Args:\n",
    "        state_file (str): Path to the state file. Defaults to \"page_state.json\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the last fetched page number for each category.\n",
    "    \"\"\"\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    return {}\n",
    "\n",
    "def save_page_state(page_state, state_file=\"page_state.json\"):\n",
    "    \"\"\"\n",
    "    Save the page state to a file to track which pages have been crawled.\n",
    "\n",
    "    Args:\n",
    "        page_state (dict): A dictionary containing the last fetched page number for each category.\n",
    "        state_file (str): Path to the state file. Defaults to \"page_state.json\".\n",
    "    \"\"\"\n",
    "    with open(state_file, \"w\") as file:\n",
    "        json.dump(page_state, file)\n",
    "\n",
    "def fetch_images_from_google_image(category, num_results, state_file=\"page_state.json\", api_key=GI_API_KEY, search_engine_id=GI_SEARCH_ENGINE_ID):\n",
    "    \"\"\"\n",
    "    Fetch image URLs for a given category using the Google Custom Search API, starting from the last saved page state.\n",
    "\n",
    "    Args:\n",
    "        category (str): The search category or query term.\n",
    "        num_results (int): The total number of image URLs to fetch.\n",
    "        state_file (str, optional): Path to the state file.\n",
    "        api_key (str, optional): The API key for Google Custom Search.\n",
    "        search_engine_id (str, optional): The search engine ID for Google Custom Search.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of image URLs fetched for the given category.\n",
    "    \"\"\"\n",
    "    page_state = load_page_state(state_file)\n",
    "    start = page_state.get(category, 1) \n",
    "\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    image_urls = []\n",
    "\n",
    "    while len(image_urls) < num_results:\n",
    "        params = {\n",
    "            'key': api_key,\n",
    "            'cx': search_engine_id,\n",
    "            'q': category,\n",
    "            'searchType': 'image',\n",
    "            'num': min(10, num_results - len(image_urls)),\n",
    "            'start': start,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        items = data.get('items', [])\n",
    "        if not items:\n",
    "            break\n",
    "        image_urls.extend([item['link'] for item in items])\n",
    "        start += len(items)\n",
    "\n",
    "    # Update and save page state\n",
    "    page_state[category] = start\n",
    "    save_page_state(page_state, state_file)\n",
    "\n",
    "    return image_urls[:num_results]\n",
    "\n",
    "def fetch_images_from_pexels(category, num_results, state_file=\"pexels_page_state.json\", api_key=PEXELS_API_KEY):\n",
    "    \"\"\"\n",
    "    Fetch image URLs for a given category using the Pexels API, starting from the last saved page state.\n",
    "\n",
    "    Args:\n",
    "        category (str): The search category or query term.\n",
    "        num_results (int): The total number of image URLs to fetch.\n",
    "        state_file (str, optional): Path to the state file. Defaults to \"pexels_page_state.json\".\n",
    "        api_key (str, optional): The API key for Pexels API.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of image URLs fetched for the given category.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": api_key}\n",
    "    base_url = \"https://api.pexels.com/v1/search\"\n",
    "\n",
    "    # Load page state\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, \"r\") as file:\n",
    "            page_state = json.load(file)\n",
    "    else:\n",
    "        page_state = {}\n",
    "\n",
    "    page = page_state.get(category, 1)\n",
    "    image_urls = []\n",
    "\n",
    "    while len(image_urls) < num_results:\n",
    "        params = {\n",
    "            \"query\": category,\n",
    "            \"per_page\": min(80, num_results - len(image_urls)), \n",
    "            \"page\": page,\n",
    "        }\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        photos = data.get(\"photos\", [])\n",
    "        if not photos: \n",
    "            break\n",
    "\n",
    "        # Collect image URLs\n",
    "        image_urls.extend([photo[\"src\"][\"original\"] for photo in photos])\n",
    "        page += 1  # Move to the next page\n",
    "\n",
    "    # Save the updated page state\n",
    "    page_state[category] = page\n",
    "    with open(state_file, \"w\") as file:\n",
    "        json.dump(page_state, file)\n",
    "\n",
    "    return image_urls[:num_results]\n",
    "\n",
    "def download_images_with_deduplication(urls, folder, db_path=\"image_embeddings.db\"):\n",
    "    \"\"\"\n",
    "    Downloads images from a list of URLs, checks for duplicates using embeddings, \n",
    "    and saves unique images to the specified folder.\n",
    "\n",
    "    Args:\n",
    "        urls (list[str]): List of image URLs to download.\n",
    "        folder (str): Folder where images will be saved.\n",
    "        db_path (str, optional): Path to the SQLite database for storing and checking image embeddings. Defaults to \"image_embeddings.db\".\n",
    "\n",
    "    The function creates the folder if it does not exist, downloads each image, generates\n",
    "    an embedding, and checks for duplicates against existing embeddings in the database. \n",
    "    If an image is not similar to existing ones, it is saved to the folder, and its \n",
    "    embedding is saved to the database. Logs info messages for successful downloads and \n",
    "    duplicates skipped, and error messages for download failures.\n",
    "    \"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    conn = init_database(db_path)\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Save image temporarily for embedding\n",
    "            temp_image_path = os.path.join(folder, \"temp.jpg\")\n",
    "            with open(temp_image_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            # Generate embedding for the image\n",
    "            embedding = get_image_embedding(temp_image_path)\n",
    "\n",
    "            # Check for similarity\n",
    "            if is_similar_to_existing(conn, embedding):\n",
    "                logging.info(f\"Duplicate image skipped: {url}\")\n",
    "                os.remove(temp_image_path)\n",
    "            else:\n",
    "                # Save the image and its embedding\n",
    "                parsed_url = urlparse(url)\n",
    "                file_name = os.path.basename(parsed_url.path)\n",
    "                if not os.path.splitext(file_name)[1]:\n",
    "                    content_type = response.headers.get('Content-Type', '')\n",
    "                    ext = mimetypes.guess_extension(content_type.split(';')[0]) if content_type else '.jpg'\n",
    "                    file_name += ext\n",
    "                image_path = os.path.join(folder, file_name)\n",
    "                os.rename(temp_image_path, image_path)\n",
    "                save_embedding_to_db(conn, url, embedding)\n",
    "                logging.info(f\"Downloaded: {image_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def fetch_and_save_images(categories, num_results_per_category, \n",
    "                          base_folder=\"images\", db_path=\"image_embeddings.db\", state_file=\"page_state.json\", type=None):\n",
    "    \"\"\"\n",
    "    Fetches image URLs for each category, downloads the images with deduplication, \n",
    "    and saves them into a structured folder hierarchy while keeping track of crawled pages.\n",
    "\n",
    "    Args:\n",
    "        categories (list[str]): List of categories to fetch images for.\n",
    "        num_results_per_category (int): Number of image results to fetch per category.\n",
    "        base_folder (str, optional): Base folder where images will be saved. Defaults to \"images\".\n",
    "        db_path (str, optional): Path to the SQLite database for storing image embeddings. Defaults to \"image_embeddings.db\".\n",
    "        state_file (str, optional): Path to the state file. Defaults to \"page_state.json\".\n",
    "        type (str, optional): Type of image source (\"Google Image\" or \"pexels\"). \n",
    "    \"\"\"\n",
    "    # Create the base folder if it does not exist\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    \n",
    "    # Loop through each category\n",
    "    for category in categories:\n",
    "        # Create category-specific folder\n",
    "        category_folder = os.path.join(base_folder, category)\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "        # Fetch image URLs based on the selected source type\n",
    "        if type == 'Google Image':\n",
    "            # Fetch URLs using Google Custom Search API\n",
    "            fetched_urls = fetch_images_from_google_image(category, num_results_per_category, state_file=state_file)\n",
    "            logging.info(f\"Fetched {len(fetched_urls)} URLs from Google Image for category '{category}'.\")\n",
    "\n",
    "        elif type == 'Pexels':\n",
    "            # Fetch URLs using Pexels API\n",
    "            fetched_urls = fetch_images_from_pexels(category, num_results_per_category, state_file=state_file)\n",
    "            logging.info(f\"Fetched {len(fetched_urls)} URLs from Pexels for category '{category}'.\")\n",
    "\n",
    "        else:\n",
    "            # Log an error or return if the type is not recognized\n",
    "            logging.error(f\"Unrecognized image source type: {type}. Please specify 'Google Image' or 'pexels'.\")\n",
    "            return\n",
    "\n",
    "        # Download and save images, avoiding duplicates\n",
    "        download_images_with_deduplication(fetched_urls, category_folder, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.807609Z",
     "iopub.status.busy": "2025-01-16T17:16:20.807242Z",
     "iopub.status.idle": "2025-01-16T17:16:20.829365Z",
     "shell.execute_reply": "2025-01-16T17:16:20.828391Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.807581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    # Kitchen\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\n",
    "\n",
    "    # Indoor\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\",\n",
    "    \n",
    "    # Vehicle\n",
    "    \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "\n",
    "    # Animal\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def daily_run():\n",
    "    #input_folder = \"/kaggle/input/vqa-image\"\n",
    "    #output_folder = \"/kaggle/working/vqa-image-updated\"\n",
    "    #output_folder_new = \"/kaggle/working/vqa-image-updated/images\"\n",
    "    \n",
    "    db_file = \"image_embeddings.db\"\n",
    "\n",
    "    num_results_per_category = 500\n",
    "    gi_state_file=\"page_state_gi.json\"\n",
    "    pexels_state_file=\"page_state_pexels.json\"\n",
    "    \n",
    "    # Types\n",
    "    p = 'Pexels'\n",
    "    gi = 'Google Image'\n",
    "\n",
    "    db_path = copy_existing_data(input_folder, output_folder, db_file)\n",
    "    \n",
    "    conn = init_database(db_path)\n",
    "\n",
    "    fetch_and_save_images(categories=categories,\n",
    "                      num_results_per_category=num_results_per_category,\n",
    "                      base_folder=output_folder_new,\n",
    "                      db_path=db_path,\n",
    "                      state_file=pexels_state_file,type=p)\n",
    "\n",
    "    zip_path = shutil.make_archive(\"vqa-image-updated\", 'zip', output_folder)\n",
    "    logging.info(f\"Updated dataset zipped at {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T17:16:20.830924Z",
     "iopub.status.busy": "2025-01-16T17:16:20.830533Z",
     "iopub.status.idle": "2025-01-16T17:16:20.851595Z",
     "shell.execute_reply": "2025-01-16T17:16:20.850386Z",
     "shell.execute_reply.started": "2025-01-16T17:16:20.830889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def first_run():\n",
    "    num_results_per_category = 500\n",
    "    db_path=\"image_embeddings.db\"\n",
    "    gi_state_file=\"page_state_gi.json\"\n",
    "    pexels_state_file=\"page_state_pexels.json\"\n",
    "    base_folder=\"images\"\n",
    "    \n",
    "    # Types\n",
    "    p = 'Pexels'\n",
    "    gi = 'Google Image'\n",
    "\n",
    "    fetch_and_save_images(categories=categories,\n",
    "                      num_results_per_category=num_results_per_category,\n",
    "                      base_folder=base_folder,\n",
    "                      db_path=db_path,\n",
    "                      state_file=pexels_state_file,type=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# daily_run()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6494681,
     "sourceId": 10489605,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
